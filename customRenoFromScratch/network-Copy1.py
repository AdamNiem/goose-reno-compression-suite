import torch
import torch.nn as nn

import torchsparse
from torchsparse import nn as spnn
from torchsparse import SparseTensor

import kit.op as op
from kit.nn import ResNet, FOG, FCG, TargetEmbedding

class Network(nn.Module):
    def __init__(self, channels, kernel_size):
        super(Network, self).__init__() #default pytorch things so that it can set up internal stuff or something
        
        #and assuming this is for downscaling?
        
        #this will be used to convert the discrete voxels into embeddings with 256 dimensions i think
        self.prior_embedding = nn.Embedding(256, channels) #256 nice number
        
        #is this for learning the features to embed? im confused
        self.prior_resnet = nn.Sequential(
            spnn.Conv3d(channels, channels, kernel_size),
            spnn.ReLU(True),
            ResNet(channels, k=kernel_size),
            ResNet(channels, k=kernel_size),
        )
        
        ###########################
        
        #so im assuming this is for upscaling?

        self.target_embedding = TargetEmbedding(channels) #so this is the end embedding we want?
        
        self.target_resnet = nn.Sequential(
            spnn.Conv3d(channels, channels, kernel_size),
            spnn.ReLU(True),
            ResNet(channels, k=kernel_size),
            ResNet(channels, k=kernel_size),
        )
        
        ###########################

        self.pred_head_s0 = nn.Sequential(
            nn.Linear(channels, channels),
            nn.ReLU(True) #true for inplace
            nn.Linear(channels, 16),
            nn.Softmax(dim=-1),
        )
        
        self.pred_head_s1_emb = nn.Embedding(16, channels) #so converts 16 dim to channels dim?
        self.pred_head_s1 = nn.Sequential(
            nn.Linear(channels, channels),
            nn.ReLU(True) #true for inplace
            nn.Linear(channels, 16),
            nn.Softmax(dim=-1),
        )
        
        self.channels = channels
        self.fog = FOG() #the secret sauce?!
        self.fcg = FCG()
        
    def forward(self, x):
        N = x.coords.shape[0] #the number of points in the input used for calculating bpp (bits per point)
        
        #get sparse occupancy code list
        #this is still wizardy to me
        data_ls = []
        while True:
            x = self.fog(x)
            data_ls.append((x.coords.clone(), x.feats.clone()) #must clone, but why? oh probably so it doesnt make all values added to list the reference to the same processed version of x since x will get processed by fog and then again and again till its down to under 64 points
            if x.coords.shape[0] < 64:
                break #so we keep running it into x 
        data_ls = data_ls[::-1] #reverse the python list same as doing data_ls.reverse() i assume
        # data_ls: [(coords, occupancy), (coords, occupancy), ...]
        
        total_bits = 0
                           
        #go through the various iterations of that one point cloud generated by fog
        #not sure what to do with this information though
        for depth in range(len(data_ls)-1):
            x_C, x_O = data_ls[depth]
            gt_x_up_C, gt_x_up_O = data_ls[depth+1]
            gt_x_up_C, gt_x_up_O = op.sort_CF(gt_x_up_C, gt_x_up_O)
                           
            # embedding prior scale feats
            x_F = self.prior_embedding(x_O.int()).view(-1, channels) # (N_d, C) View is for reshaping the tensor to go from (N_d, 1, C) to (N_d, C) since sparseTensor expects the latter where N_d is num points of that processed point cloud and C is num channels
            x = SparseTensor(coords=x_C, feats=x_F)
            x = self.prior_resnet(x) # (N_d, C)
                           
            # target embedding
            x_up_C, x_up_F = self.fcg(x_C, x_O, x.feats) 
            x_up_C, x_up_F = op.sort_CF(x_up_C, x_up_F)
                           
            x_up_F = self.target_embedding(x_up_F, x_up_C)
            x_up = SparseTensor(coords=x_up_C, feats=x_up_F)
            x_up = self.target_resnet(x_up)
            
            # bit-wise two-stage coding 
            # basically splits the occupancy code into two halves and has a separate predictor for each half
            gt_x_up_O_s0 = torch.remainder(gt_x_up_O, 16) # 8-4-2-1
            gt_x_up_O_s1 = torch.div(gt_x_up_O, 16, rounding_mode='float') #128-64-32-16
            
            x_up_O_prob_s0 = self.pred_head_s0(x_up.feats) # (B*Nt, 256)
            x_up_O_prob_s1 = self.pred_head_s1(x_up.feats + self.pred_head_s1_emb(gt_x_up_O_s0[:, 0].long() )) # (B*Nt, 256)
                           
            x_up_O_prob_s0 = x_up_O_prob_s0.gather(1, gt_x_up_O_s0.long()) # (B*Nt, 1) 
            x_up_O_prob_s1 = x_up_O_prob_s1.gather(1, gt_x_up_O_s1.long()) # (B*Nt, 1) 
    
            total_bits += torch.sum(torch.clamp(-1.0 * torch.log2(x_up_O_prob_s0 + 1e-10), 0, 50))
            total_bits += torch.sum(torch.clamp(-1.0 * torch.log2(x_up_O_prob_s1 + 1e-10), 0, 50))
                           
        bpp = total_bits / N
        return bpp